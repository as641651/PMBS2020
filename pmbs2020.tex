\renewcommand{\encodingdefault}{OT1}
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{multirow}
\usepackage[Procedure]{algorithm}
%\usepackage{algorithmic}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\definecolor{orange}{rgb}{1,0.5,0}
\newcommand{\p}[1]{{\color{blue} Pdj: #1}}
\newcommand{\as}[1]{{\color{red} a: #1}}
\newcommand{\ar}[1]{{\color{red}#1}}
\usepackage{ulem}  % strike through  \sout{ }
\usepackage{url}
\DeclareMathOperator*{\argminA}{arg\,min}
\usepackage{booktabs}
\begin{document}

\title{Robust Ranking of Linear Algebra Algorithms via Relative Performance\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
\thanks{Financial support from the Deutsche Forschungsgemein- schaft (German Research Foundation) through grants GSC 111 and IRTG 2379 is gratefully acknowledged.}
}

\author{\IEEEauthorblockN{Aravind Sankaran}
\IEEEauthorblockA{\textit{AICES} \\
\textit{RWTH Aachen University}\\
Aachen, Germany \\
aravind.sankaran@rwth-aachen.de}
%\and
% \IEEEauthorblockN{Christos Psarras}
%\IEEEauthorblockA{\textit{AICES} \\
%	\textit{RWTH Aachen University}\\
%	Aachen, Germany \\
%psarras@aices.rwth-aachen.de}
\and
\IEEEauthorblockN{Paolo Bientinesi}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Ume\r{a} Universitet}\\
Ume\r{a}, Sweden \\
pauldj@cs.umu.se}
%\and
%\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
}

\maketitle

\begin{abstract}

  For a given linear algebra problem, we consider those solution algorithms that are mathematically equivalent to one
  another, and that mostly consist of a sequence of calls to kernels from optimized libraries such as BLAS and
  LAPACK. Although equivalent (at least in exact precision), those algorithms typically exhibit significant differences
  in terms of performance, and naturally, we are interested in finding the fastest one(s). In practice, we often observe
  that multiple algorithms yield comparable performance characteristics. Therefore, we aim to identify the subset of algorithms that are reliably faster than the rest. To this end, instead of quantifying the performance of an
  algorithm in absolute terms, we present a measurement-based approach that assigns a relative score to the algorithms
  in comparison to one another. The relative performance is encoded by sorting the algorithms based on pair-wise
  comparisons and ranking them into equivalence classes, where more than one algorithm can obtain the same rank. We show
  that the relative performance leads to robust identification of the fastest algorithms, that is, reliable identifications even with noisy system
  conditions.
\end{abstract}

\begin{IEEEkeywords}
\textbf{performance analysis, performance modelling, benchmarking, sampling}
\end{IEEEkeywords}


\section{Introduction}


Given a set $\mathcal{A}$ of mathematically equivalent linear algebra algorithms, we aim to identify the subset
$\mathcal{F} \subseteq \mathcal{A}$ containing all those algorithms that are ``equivalently'' fast to one another, and
``noticeably'' faster than the algorithms in the subset $\mathcal{A}/\mathcal{F}$. We will clarify the meaning of ``equivalent'' and
``noticeable'' shortly; for now, we simply state that in order to identify $\mathcal{F}$, 
we develop an approach that assigns a higher score to the algorithms in
$\mathcal{F}$ compared to those in $\mathcal{A}/\mathcal{F}$. Instead of aiming for a value that captures the quality of an algorithm in absolute terms, we seek to compute a relative score that compares the current algorithm with respect to the fastest algorithm(s) in $\mathcal{A}$. We refer to such scores as ``relative performance estimates''.

It is well known that execution times are influenced by many factors, and that repeated measurements, even with same input data and same cache conditions, often result in different execution times~\cite{peise2014cache,hoefler2010characterizing,peise2012performance}. Therefore, finding the best algorithm is a task that involves comparing distributions of execution time.
%
In common practice, time measurements are summarized into statistical estimates (such as minimum or median execution time), which are then used to compare algorithms~\cite{peise2019elaps}. But when system noise begins to have significant impact on program execution, it becomes difficult to summarize the performance into a single number; as a consequence, the comparisons are not consistent when the time measurements are reproduced, and this leads to inconsistency in the ranking of algorithms.
In order for one algorithm to be better (or worse) than the other, there should be ``noticeable`` difference in their distributions (Figure \ref{fig:diff}). On the other hand, the performance of algorithms are comparable if their distributions are ``equivalent`` or have significant overlap (Figure \ref{fig:eq}). 
%
Therefore, the result of comparing two algorithms can fall into one of the three categories - better, worse or equivalent. 
In this paper, we use this three-way comparison to cluster the set of algorithms $\mathcal{A}$ into performance classes and construct a ranking that is consistent (or robust) despite noisy system conditions. 
%
\begin{figure}[h!]
	\centering
		\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=1\linewidth]{fig/dif.png}
		\caption{Algorithms having noticeable difference in distributions}
		\label{fig:diff} 
	\end{subfigure}

	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=1\linewidth]{fig/eq.png}
		\caption{Algorithms having significant overlap of distributions}
		\label{fig:eq}
	\end{subfigure}
	
	\caption{Five hundred measurements of three solution algorithms (Yellow, Blue, Red) for the Ordinary Least Square Problem}
	\label{fig:1}
\end{figure}

The algorithms in $\mathcal{A}$ represent different, alternative ways of computing the same mathematical expression. In
exact arithmetic, those algorithms would all return the same quantity.  For instance,  in the expression $y_k := H^{T}y
+ (I_n - H^{T}H)x_k$, which appears in an image restoration application\cite{tirer2018image}, if the product $H^TH$ is
computed explicitly, the code would perform a $\mathcal{O}(n^3)$ matrix-matrix multiplication; by contrast, by applying
distributivity,\footnote{In general, distributivity does not always lead to lower FLOP count.} one can rewrite this
assignment as $y_k := H^{T}(y - Hx_k) + x_k$, obtaining an alternate algorithm which computes the same expression by
using only matrix-vector multiplications, for a cost of $\mathcal{O}(n^2)$. In this example, the two algorithms differ
in the order of magnitude of floating point operations (FLOPs), hence noticeable difference in terms of execution times are naturally expected. However, two algorithms may differ significantly in execution times even if they perform the same number of FLOPs, and it is even possible that higher FLOP counts result in faster executions~\cite{barthels2019linnea}.

%As mentioned earlier, to determine which of two algorithms is the fastest, their distributions of execution times should be compared.  The result will depend on the specific sizes of input matrices (operand sizes). Figure [1] shows the distributions of the two algorithms for image restoration for two different dimensions of $H$. When $H$ is small, both the algorithms are equally fast and we want to assign them both to $\mathcal{F}$. If the dimension of $H$ is large, the gains of distributivity starts showing and now only the second algorithm should be in $\mathcal{F}$. 

In practice, one has to consider and compare more than just two algorithms.  For instance, for the generalized least square problem $y := (X^TS^{-1}X)^{-1}X^{T}S^{-1}z$, it is possible to find more than 100 different algorithms that compute the solution and that differ not more than 1.4x in terms of FLOP count.\footnote{Julia code for 100 equivalent algorithms of the Generalized Least Square problem is available here github.com/as641651/Relative-Performance/tree/master/Julia-Code/GLS/Julia/generated}  
%the product $H^{T}H$ could be computed both by the kernel $gemm$, which implements matrix matrix multiplications, and by calling a different kernel $syrk$, which instead computes a symmetric rank-k update.
All these different algorithms arise by considering properties of input matrices (such as symmetric positive definite, lower or upper triangular etc.), different paranthesizations for matrix chains, identifying common sub-expressions, etc \cite{psarras2019linear}. In this work, we consider solution algorithms generated by the Linnea framework~\cite{barthels2019linnea}. For a given mathematical expression with fixed operand size, Linnea generates a family of algorithms (in the form of Julia code~\cite{julia}) consisting of (mostly but not limited to) sequences of BLAS or LAPACK calls.

\textbf{\textit{Contribution:} }The efficient computation of mathematical expressions is critical not only for complex
simulations but also for time-critical computations done on resource-constrained hardware \cite{towardsEdgeComputing}
(e.g., instantaneous object detection for autonomous driving applications \cite{connectedvehicles}). A framework that
chooses the best algorithm should take into consideration at-least the systematic noise factors, such as  regular
interrupts, impacts of  other applications that are simultaneously sharing the resources, etc. Moreover, we observed that
multiple algorithms can yield similar performance profiles. Therefore,  we develop a methodology for robust identification of not one, but a set of  fast algorithms for a given operational setting. To this end, we use the bootstrapping technique~\cite{bootstrap} to rank the set of equivalent algorithms $\mathcal{A}$ into clusters, where more than one algorithm can obtain the same rank.

\textit{\textbf{Organization}}: In Sec.~\ref{sec:rel}, we highlight the challenges and survey the state of art. In
Secs.~\ref{sec:torel} and~\ref{sec:met}, we introduce the idea behind relative performance, and describe a methodology
to obtain relative scores, respectively; the methodology is then evaluated in Sec.~\ref{sec:exp}.
 Finally, in Sec.~\ref{sec:con}, we
draw conclusions and discuss the need for relative performance modelling. 
  

\section{Related Works}
\label{sec:rel}

Since linear algebra expressions are at the heart of a myriad of
applications in both scientific computing and data science,
their efficient computation is essential. Users rely more and more on high-level languages and
environments such as Julia~\cite{julia}, Matlab~\cite{MatlabOTB}, and Tensorflow~\cite{tensorflow}, which accept linear
algebra expressions as input, and internally map them to a sequence of calls to kernels from optimized libraries such as
BLAS and LAPACK. This is precisely the type of algorithms we consider in this work. The problem of mapping a target
expression to sequences of library calls is known as the Linear Algebra Mapping Problem (LAMP)~\cite{psarras2019linear};
typical problem instances have many mathematically equivalent solutions, and these languages have to select the best
one. However, it has been shown that most of them choose algorithms that are sub-optimal in terms of performance~\cite{psarras2019linear,barthels2019linnea}.

A general approach to identify the best candidate is by ranking them according to their predicted performance.
For linear algebra computations, the
most common performance metric to be used as performance predictor is the FLOP count, even though it is known that the
number of FLOPs is not always a direct indicator of the fastest code~\cite{barthels2019linnea}. In addition to FLOPs,
Iakymchuk et al. model the performance analytically based on memory access
patterns~\cite{iakymchuk2012modeling,iakymchuk2011execution}; while their models can represent program execution
accurately, their construction needs not only a deep understanding of the processor architecture, but also a detailed analysis of kernel implementations, which are not easily available.

Due to system noise~\cite{hoefler2010characterizing}, cache effects~\cite{peise2014cache}, behaviour of
collective communications \cite{agarwal2005impact}, and several other factors, 
even an ideal performance metric capable of capturing the execution time of the program would still lack in
reproducibility. The distribution of execution times obtained by repeated measurements of a program is known to lack in textbook statistical behaviours and it is not realistic to eliminate the performance variations entirely~\cite{robustbenchmarking,trackingPerfVariation,statiscalperfCompare}. 

Calotoiu et al. in \cite{calotoiu2013} use semi-analytical models, which are designed based on the understanding of
computer programs behaviour in practical scenarios. Time measurements are used to determine coefficients of their
models. The focus is mainly on modelling scalability of a specific algorithm and not on identifying the best among
several alternatives for a given number of processors. For a given operational setting, our relative estimates do not
capture the speed up of one algorithm over the other; instead, they separate the fastest algorithm(s) from the rest.
% without an accurate assessment of its(their) performance.

The problem we are tackling was also considered by Peise et al.~\cite{peise2012performance}.
Their approach is to model the performance (which is purely measurement-based) of BLAS calls that form the building blocks of
a linear algebra program and then hierarchically compose them to predict statistical estimates of execution time for an
entire algorithm. These estimates are then used to rank different variants of algorithms. However, they do not attempt
to detect if two algorithms are performance-equivalent and hence their rankings can be inconsistent (non-robust) to
repetitions. The approach we propose aims to make the performance metric more interpretable and robust by capturing the equivalence of two algorithms.  

Guidelines for interpretable benchmarking using statistical approaches are discussed by Hoefler et
al.~\cite{hoefler2015scientific}, who recommend to compute confidence intervals or conduct tests for significant
differences to decide whether or not two
algorithms are performance-equivalent. Their focus is mainly on obtaining (absolute) performance estimates for an
algorithm in isolation, whose interpretability depends on the number of measurements (sample size) used to compute such
estimates. While Ritter et al.~\cite{ritter2020learning} proposed cost effective sampling strategies to build
performance models, the focus is on identifying scalability bugs. By contrast, we aim to compute interpretable
``relative" estimates of algorithms in comparison to one and other, which are not influenced much by sample size (see
Sec.~\ref{sec:exp} B).
 

\section{Towards Relative Performance}
\label{sec:torel}
%The ranking methodology described in section 2 establishes a notion of distance (in terms of performance) between the algorithms in $\mathcal{A}$.
Fundamentally, we want to compute a score for every algorithm in $\mathcal{A}$ that indicates its chance of being the fastest algorithm. As an example, consider the distributions of execution times shown in Fig.~\ref{fig:1} for three different algorithms (Red, Blue and
Yellow) that solve the Ordinary Least Square problem:\footnote{The pseudocode of solution algorithms are shown in Appendix A. For the sake of
  explanation, we only consider three solution algorithms; for this problem, Linnea generates many more than three,
  as explained in Sec.~4 of~\cite{barthels2019linnea}.} $(X^TX)^{-1}X^{T}y$. 
The distributions were obtained by measuring 500 times the execution time of every algorithm $\mathbf{a}_j \in \mathcal{A}$  ($|\mathbf{a}_j| = 500$).
In order to ensure that the time measurements are unbiased of system noise, we adapt the following procedure: Let the set of all executions be $\mathcal{E} = \mathbf{a}_1 \oplus \mathbf{a}_2 \oplus \mathbf{a}_3$, where $\oplus$
is a concatenation operation. The set $\mathcal{E}$ is shuffled and the executions are timed. Every execution $e \in
\mathcal{E}$ is run twice and only the second measurement is considered; the first measurement is mostly an outlier due
to loading and caching of data and instructions\cite{peise2019elaps}. 

A ``straightforward'' approach to relatively score algorithms consists in finding the minimum (or median, or mean) execution time of every algorithm $\mathbf{a}_j \in \mathcal{A}$ and then use that to construct a ranking~\cite{peise2012performance}. By doing this, every algorithm
  would obtain a unique rank. If we choose to assign the algorithm with rank 1 to the set of fastest algorithm
  $\mathcal{F}$, then only one algorithm could be assigned to $\mathcal{F}$ although the distributions Yellow and
  Blue are nearly identical. This will lead to inconsistent rankings when all the executions $\mathcal{E}$ are
  repeated. Reproducibility of rankings is essential in order to derive mathematically sound performance
  predictions.\footnote{The modelling and prediction of relative performance is the objective of our future work, and it is out of the scope of this article.}  In order to ensure reproducibility of relative performance estimates, both Yellow and Blue should be assigned to $\mathcal{F}$. 
  To this end, one could choose the $k$-best algorithms\cite{kbest-kadioglu2011algorithm} with $k=2$, and  assign both
  Yellow and Blue to $\mathcal{F}$. However, the fixed value $k=2$  might not be
  suitable for other scenarios in which either one algorithm or more than two algorithms are superior to the rest. 


 Bootstrapping\cite{bootstrap} is a common procedure to extract more information from the distribution. Here, the ``straightforward'' approach is repeated $T$ times, and for each iteration, $K < 500$ measurements are sampled from the original 500 measurements and rankings are constructed. If an algorithm $\mathbf{a_j}$ obtains rank 1 in at least one out of $T$ iterations, it is assigned to $\mathcal{F}$, and will
 receive the score of  $c/T$, where $c \le T$ is the number of times $\mathbf{a_j}$ obtains rank 1. The steps for
 this approach, adapted to our problem, are shown in Procedure \ref{alg:fa}. For a given set of algorithms $
 \mathbf{a}_1,\mathbf{a}_2 ,\dots, \mathbf{a}_p\in \mathcal{A}$ for which the execution times are measured $N$ times each,
 Procedure \ref{alg:fa} returns the set of fastest algorithms $f_1, f_2, \dots, f_q \in  \mathcal{F}$, ($q \le p$) each with an associated score $c_1,c_2,\dots,c_q$.
 For instance, when this procedure is applied to the distributions in Fig.~\ref{fig:1} (with $T=100$ and a sample size $K=5$ ) both Yellow and Blue are assigned to $\mathcal{F}$ with scores 0.55 and 0.45 respectively. 

\begin{algorithm}
	\caption{ Get$\mathcal{F}$$(\mathcal{A})$ }
	\label{alg:fa}
	\hspace*{\algorithmicindent} \textbf{Input: } $ \mathbf{a}_1,\mathbf{a}_2 ,\dots, \mathbf{a}_p\in \mathcal{A}$ \\
	\hspace*{\algorithmicindent} \textbf{Output: } $ (f_1,c_1), (f_2, c_2), \dots, (f_q,c_q) \in \mathcal{F} \times C  $
	\begin{algorithmic}[1] 
		\State $L, C \leftarrow [ \quad ]$ \Comment{Initialize empty lists}
		\For{i = 1, $\dots$, $T$}
		\State $\mathbf{\tilde{a}_j} \leftarrow K $ values sampled from $\mathbf{a}_j \quad \forall j \in \{1,\dots,p\}$ 
%		\State sample $K$ values $\mathbf{\tilde{a}_{j}} \subset \mathbf{a_j} \quad \forall j$ \Comment{$ j \in \{1,\dots,p\}$} 
		\State $ f \leftarrow \underset{j}{\mathrm{argmin}}(\min(\mathbf{\tilde{a}}_j) | \forall j) \quad \mathbf{\tilde{a}}_j \in \mathbb{R}^{K}$
                \p{$\forall j$ is not needed, $j$ is already bound in argmin}
                \p{so the operation is: 1. take the min of the sub-samples; 2. take the min of mins; 3. return the index
                of such alg. -- I feel that a short comment is needed}
                \State append$(f,L)$
                % \State append($\tilde{\mathcal{F}},L$)
		\EndFor
		\State $\mathcal{F} \leftarrow $ unique elements in $L$ \Comment{ $q = |\mathcal{F}| $}
		\For{i = 1, $\dots$, q }
        \State $c \leftarrow$ number of occurrences of $f_i$ in $L$ $\quad f_i \in \mathcal{F}$
		\State append($c/T, C$)
		\EndFor
		\State return $(\mathcal{F} \times C)$
              \end{algorithmic}
\p{flush comments to the right margin (maybe with hfill?)}
\end{algorithm}

%Let $\mathcal{A}(x)$ be the set of all solution algorithms for certain linear algebra expression $x$. 
However, in practical applications, the execution times of algorithms are typically
measured only a few times (likely much fewer than 500 times, possibly just once), and therefore, only a snapshot of the distributions
shown in Fig.~\ref{fig:1} will be available. Procedure \ref{alg:fa} does not account for the uncertainty in measurement data in capturing the true distributions. 
%The scores obtained by Procedure \ref{alg:fa} still does have a meaningful translation to the indicate the distance between the algorithms; ideally, the scored obtained for Red and Blue should be comparable. This is because only one algorithm is assigned to $\mathcal{F}$ in every iteration. 
The recommended approach is to use test for significant differences (such as Kruskal-Walis
ANOVA~\cite{hoefler2015scientific}) between algorithm pairs, and to merge the ranks of algorithms if there is not enough
evidence of one algorithm dominating the other. In this paper, we incorporate the test for significant differences in
Procedure \ref{alg:fa} by re-defining the straightforward sorting function (in line 4 of Procedure 1) by introducing a
three-way comparison function. 

\section{Methodology}
\label{sec:met}

In order to compute the relative score for an algorithm, the first step consist of sorting all the mathematically
equivalent linear algebra algorithms $\mathbf{a}_1,\mathbf{a}_2, \dots, \mathbf{a}_p \in \mathcal{A}$ and assigning them
to one of the equivalence classes $r_1, r_2, \dots,r_w$. The number of classes (or ranks) is $w \le p$ as more than one
algorithm can obtain the same rank. For illustration, we consider an example where four algorithms
$\mathbf{a}_1,\mathbf{a}_2, \mathbf{a}_3, \mathbf{a}_4$ could be sorted and ranked as $\langle (\mathbf{a}_2,1),
(\mathbf{a}_4,1), (\mathbf{a}_1,2), (\mathbf{a}_3,2) \rangle$. 
  All the algorithms with rank 1 are assigned to the set of fastest algorithms $\mathcal{F}$. The assignments of ranks
  need not be deterministic. Hence, in the second step, we bootstrap and repeat this procedure $T$ times and all the algorithms that obtained rank 1 at least once are assigned to $\mathcal{F}$. Then, we follow the steps from line 5 - 9 in Procedure \ref{alg:fa} to compute relative scores. 

The execution time of every algorithm $\mathbf{a} \in \mathcal{A}$ is
measured $N$ times. The measurements are stored in an array of size N; hence, one can also denote
$\mathbf{a} \in \mathbb{R}^N$. We do not make any assumptions about the nature of the distributions of timings. To compare the
distributions of two algorithms $\mathbf{a_i},\mathbf{a_j} \in \mathcal{A}$, we follow the steps indicated in Procedure
\ref{alg:compare}.
%\p{``method'' is not a common word for this. ``Algorithm'', ``Procedure'', ``Test'', ... Later you call it ``function'', which is also ok}
%
The comparison yields one of three outcomes:
\begin{itemize}
\item Outcome A: algorithm $\mathbf{a_i}$ is better than $\mathbf{a_j}$ $(<)$
\item Outcome B: algorithm  $\mathbf{a_i}$ is as good as $\mathbf{a_j}$ $(\sim)$
\item Outcome C: algorithm  $\mathbf{a_i}$ is worse than $\mathbf{a_j}$ $(>)$
\end{itemize}
Outcomes A and C imply that there are noticeable differences between the distributions; outcome B implies that the
distributions are performance-wise equivalent. 
\begin{algorithm}
	\caption{Compare $(\mathbf{a_i}, \mathbf{a_j})$ }
	\label{alg:compare}
	\hspace*{\algorithmicindent} \textbf{Input: } $ \mathbf{a_i}, \mathbf{a_j} \in \mathbb{R}^{N}$ \\
	\hspace*{\algorithmicindent} \textbf{Output } $\in  \{<,>,\sim \}$
	\begin{algorithmic}[1] 
		\State $p \leftarrow 0$
		\For{m = 1, $\dots$, $M$}
        \State $\mathbf{\tilde{a}_i} \leftarrow K$ values sampled from $\mathbf{a_i}$
        \State $\mathbf{\tilde{a}_j} \leftarrow K$ values sampled from $\mathbf{a_j}$
		\State $e_i$ = min($\mathbf{\tilde{a}_i}$)
		\State $e_j$ = min($\mathbf{\tilde{a}_j}$)
		\If{$e_i \le e_j$}
		\State $p=p+1$
		\EndIf
		\EndFor
		\If{$\frac{p}{M} \geq threshold$ }
		\State return ``$<$"    \Comment{$\mathbf{a_i}$ is better than $\mathbf{a_j}$}
		\ElsIf{$\frac{p}{M} < 1 - threshold$}
		\State return ``$>$" \Comment{$\mathbf{a_i}$ is worse than $\mathbf{a_j}$}
		\Else
		\State return ``$\sim$" \Comment{$\mathbf{a_i}$ is as good as $\mathbf{a_j}$}
		\EndIf
	\end{algorithmic}
      \end{algorithm}
%\p{The level of detail is good, but the connection is missing. ``To this end'' is not linking to anything.
%  Also, use references (``one could also use other metrics'' -- as done in [ref], [ref].}  
In Procedure \ref{alg:compare}, the distributions $\mathbf{a_i}$ and $\mathbf{a_j}$ are sampled ($\mathbf{\tilde{a}_i}
\subset \mathbf{a_i}$ and $\mathbf{\tilde{a}_j} \subset \mathbf{a_j}$) with sample size $K < N$ and the minimum
execution time from the respective samples are compared. This procedure is repeated $M \ge 1$ times.
In theory, one could
also use other metrics like mean or median execution time or even combination of multiple metrics across different
comparisons. Peise et al. in \cite{peisethesis} show that minimum and median statistic are less sensitive to
fluctuations in execution time for linear algebra computations.

Let $e_i$ and $e_j$ be the minimum execution time of samples  $\mathbf{\tilde{a}_i}$ and $\mathbf{\tilde{a}_j}$,
respectively. The probability that $e_i$ is less than $e_j$ is approximated from the results of $M$ comparisons. That
is, if $e_i$ is less than $e_j$ in $p$ out of $M$ comparisons, then the empirical probability $P[e_i \le e_j]$ is
$p/M$. The outcome of the Compare function (Procedure \ref{alg:compare}) is determined based on this empirical
probability (see lines 9 - 14 in Procedure \ref{alg:compare}). It should be clear that the outcome of Compare function is not entirely
deterministic: If $p/M \to threshold$, then  $\mathbf{a_i}$ approaches the ``state of being better'' than $\mathbf{a_j}$ and
the outcome can tip over A ($<$) or B ($\sim$) when the Compare function is repeated (even for the same input distributions). Hence the comparisons need not be transitive;  $\mathbf{a_1} < \mathbf{a_2}$ and
$\mathbf{a_2} < \mathbf{a_3} $ does not imply $\mathbf{a_1} < \mathbf{a_3}$.

\textbf{\textit{Effect of} $K$:} As $K \to N$, the minimum estimates of samples ($e_i, e_j$) approximates the
minimum execution time ($m_i, m_j$) of the distributions $\mathbf{a_i}$ and $\mathbf{a_j}$, respectively, and the
comparison in line 7 of Procedure \ref{alg:compare} becomes more and more deterministic. As a consequence, $p/M \to 0
| 1$ and outcome B
($\mathbf{a}_i \sim \mathbf{a}_j$) becomes less and less likely. When $K=N$,  outcome B becomes impossible and this invalidates the advantages of bootstrapping. When $K \to 1$, the minimum estimates of samples ($e_i, e_j$) points to an instance of execution time ($t_i, t_j$) from the distribution. Then outcome B becomes more likely even for marginal overlap of distributions (as in Fig \ref{fig:diff}), especially when $threshold$ is high or $M$ is small.


\textbf{\textit{Effect of }$threshold$:} Valid values of $threshold$ for Procedure \ref{alg:compare} lie in the range $[0.5,1]$. When $threshold \to 0.5$, the outcome B ($\sim$) becomes
less and less likely (and impossible for $threshold=0.5$), while for $threshold \to 1$, the conditions for outcomes A ($<$)
and C ($>$) becomes stricter and outcome B becomes more and more likely. 

The Compare function (Procedure \ref{alg:compare}) is used in Procedure \ref{alg:sort} to sort the algorithms $\mathbf{a}_1,\mathbf{a}_2, \dots, \mathbf{a}_p \in \mathcal{A}$ and assign them to one of the equivalence classes $r_1, r_2, \dots, r_w$.
Procedure~\ref{alg:sort} illustrates the swapping procedure that uses bubble-sort\cite{bubblesort} with three-way comparisons.
\begin{algorithm}
	\caption{Sort $(\mathcal{A})$ }
	\label{alg:sort}
	\hspace*{\algorithmicindent} \textbf{Input: } $ \mathbf{a}_1,\mathbf{a}_2,\dots,\mathbf{a}_p \in \mathcal{A}$ \\
	\hspace*{\algorithmicindent} \textbf{Output: } $ \langle (\mathbf{a}_{s[1]},r_1), (\mathbf{a}_{s[2]}, r_2), \dots, (\mathbf{a}_{s[p]},r_w) \rangle $
	\begin{algorithmic}[1] 
		\State Initialize w $\leftarrow$ p
		%\State Initialize $r_1,\dots,r_w$ with consecutive ranks 
		\State Initialize $\mathbf{r}_j \leftarrow j \quad \forall j \in \{1,2,\dots,p\}$ \Comment{Alg ranks}
		\State Initialize $\mathbf{s}_j \leftarrow j \quad \forall j \in \{1,2,\dots,p\}$ \Comment{Alg index }
		%\State Initialize $\mathbf{a}_{s[j]} \leftarrow \mathbf{a}_j \quad \forall  j \in \{1,2,..,p\}$
		\For{i = 1, $\dots$, p}
		\For{j = 0, $\dots$, p-i-1}
		\State ret = Compare($\mathbf{a}_{s[j]}, \mathbf{a}_{s[j+1]}$)
		\If{$\mathbf{a}_{s[j+1]}$ is better than $\mathbf{a}_{s[j]}$}
		\State Swap $\mathbf{s}_{j}$ and $\mathbf{s}_{j+1}$ \label{lst:swap}
		\If{$r_{j+1} = r_j$} \label{lst:h1}
		\If{$r_{j-1} \ne r_j$ or $j=0$}
		\State Increment ranks $r_{j+1}, \dots, r_p$ by 1 \label{lst:h2}
		\EndIf
		\Else
		\If{$r_{j-1} = r_j$ and $j\ne0$} \label{lst:gg1}
		\State Decrement ranks $r_{j+1}, \dots, r_p$ by 1 \label{lst:gg2}
		\EndIf
		\EndIf
		\ElsIf{$\mathbf{a}_{s[j+1]}$ is as good as $\mathbf{a}_{s[j]}$} \label{lst:ag1}
		\If{$r_{j+1} \ne r_j$}
		\State Decrement ranks $r_{j+1}, \dots, r_p$ by 1 \label{lst:ag2}
		\EndIf
		\ElsIf{$\mathbf{a}_{s[j]}$ is better than $\mathbf{a}_{s[j+1]}$}
		\State Leave the ranks as they are
		\EndIf		
		\EndFor
		\EndFor
		\State return $\langle (\mathbf{a}_{s[1]},r_1), \dots, (\mathbf{a}_{s[p]},r_w) \rangle$
              \end{algorithmic}
            \end{algorithm}
All the algorithms $\mathbf{a}_1, \mathbf{a}_2, \dots, \mathbf{a}_p \in \mathcal{A}$ are first initialized with consecutive
ranks $r_j \leftarrow j$ (where $j \in\{1,2,\dots,p\}$) according to the order in which they are given as input $\langle 1,2,\dots,p \rangle
$.
Initially, the number of equivalence classes $w$ is same as the number of algorithms $p$. As more than one algorithm
can obtain the same rank, we use different index array to map to the position of algorithms in the sorted sequence
$\mathcal{S} : \langle \mathbf{s}_1, \mathbf{s}_2, \dots, \mathbf{s}_p \rangle$. 
Procedure \ref{alg:sort} sorts an initial sequence $\langle (\mathbf{a}_1,1), \dots, (\mathbf{a}_p,p) \rangle$ and returns $\langle (\mathbf{a}_{s[1]},1), \dots, (\mathbf{a}_{s[p]},w) \rangle$; for instance, if $\langle (\mathbf{a}_{2},1), (\mathbf{a}_{4},1) , (\mathbf{a}_{1},2), (\mathbf{a}_{3},2) \rangle$ is the returned sequence, $\mathbf{s}_1$ (or $\mathbf{s}[1]$) is the index of the first algorithm, which is 2, and this algorithm obtains rank 1. $\mathbf{s}_2$ is the second index, which is 4 with rank 1 and so on.
%In the beginning, $s_j \leftarrow j$ and $
%\mathbf{a}_{s[j]} \leftarrow \mathbf{a}_j$.
%\p{I got lost. What is ${a}_{s[j]}$? What do the [ ] mean?} 
%Thus, Procedure \ref{alg:sort} sorts the initial sequence $\langle (\mathbf{a}_1,1), \dots, (\mathbf{a}_p,p) \rangle$ and returns $\langle (\mathbf{a}_{s[1]},1), \dots, (\mathbf{a}_{s[p]},w) \rangle$.
%\p{this is not immediately clear to me. Then it has to be explained}
% and slots $s_j \leftarrow j$ , $ \mathbf{a}_{s[j]} \leftarrow \mathbf{a}_j$ where $j \in\{1,2,..p\}$; as more than one algorithm can obtain the same rank, we use different indices for positions (slots) of the algorithms ($s_j$) in the sorted sequence and ranks ($r_j$).  
 Starting from the first element of the initial sequence,  bubble sort\cite{bubblesort} compares adjacent algorithms and swaps
 indices if the algorithm occurring later in the sequence $\mathbf{a}_{s[j+1]}$ is better than its predecessor algorithm
 $\mathbf{a}_{s[j]}$ and the ranks are updated. To understand the rank update rules, consider the example in Fig
 \ref{fig:sort}, which shows the intermediate steps while sorting the distributions of four algorithms $\mathbf{a}_1,
 \mathbf{a}_2, \mathbf{a}_3,\mathbf{a}_4$ (initialized with ranks 1,2,3,4 respectively). All possible update rules that
 one might encounter appear in one of the intermediate steps of this example.
%If an algorithm $\tilde{s}_{j+1}$ is better than its previous algorithm in sequence $\tilde{s}_j$, then the position index of the two algorithms are swapped.

\begin{enumerate}
	\item 
          In the first pass of bubble sort, pair-wise comparison of adjacent algorithms are done starting from the first
          element in sequence. Currently, the sequence is $\langle \mathbf{a}_1, \mathbf{a}_2, \mathbf{a}_3, \mathbf{a}_4 \rangle$.
          As a first step, algorithms $\mathbf{a}_1$ and $\mathbf{a}_2$ are compared, and 
          $\mathbf{a}_2$ ends up being faster. As the slower algorithm should be shifted towards the end of sequence, $\mathbf{a}_1$ and $\mathbf{a}_2$ swap positions  (line \ref{lst:swap} in
          Procedure \ref{alg:sort}). Since all the algorithms still have unique ranks, $\mathbf{a}_2$ and
          $\mathbf{a}_1$ also exchange their ranks and no special rules for updating ranks are applied.
          So, $\mathbf{a}_2$ and $\mathbf{a}_1$ receive rank 1 and 2, respectively.
	
	\item Next, algorithm $\mathbf{a}_1$ is compared with its successor $\mathbf{a}_3$;
          since they are just as good as one another, 
          no swap takes place. Now, the rank of $\mathbf{a}_3$ should also indicate that it is as good as $\mathbf{a}_1$;
          so $\mathbf{a}_3$ is given the same rank as $\mathbf{a}_1$ 
            and the rank of $\mathbf{a_4}$ is corrected by
         decrementing 1. (line \ref{lst:ag1}-\ref{lst:ag2} in Procedure \ref{alg:sort}). Hence $\mathbf{a}_1$ and $\mathbf{a}_3$ have rank 2 and $\mathbf{a}_4$ is corrected to rank 3.
	
       \item
         In the last comparison of the first sweep of bubble sort, Algorithm $\mathbf{a}_4$ results to be better than $\mathbf{a}_3$, so their positions are
          swapped. However, ranks cannot be exchanged the same way it happened in step 1,
          as $\mathbf{a}_3$ would receive rank 3, even though it was evaluated to be
          as good as $\mathbf{a}_1$, which has rank 2 (recall from step 2: $\mathbf{a}_3 \sim \mathbf{a}_1$). So
          instead of changing the rank of $\mathbf{a}_3$, the rank of $\mathbf{a}_4$ is improved (line \ref{lst:gg1} -
          \ref{lst:gg2} in Procedure \ref{alg:sort}). Thus  $\mathbf{a}_1$,  $\mathbf{a}_4$ and  $\mathbf{a}_3$ are
          given rank 2, but $\mathbf{a}_4$ is now earlier in the sequence than $\mathbf{a}_3$. This indicates that there
          is an inherent ordering even when algorithms share the same rank.
          Every pass of bubble sort pushes the slowest algorithm to the end of the sequence.
          At this point, the current sequence is $\langle \mathbf{a}_2, \mathbf{a}_1, \mathbf{a}_4, \mathbf{a}_3 \rangle$.
		
	\item
          In the second pass of bubble sort the last algorithm in sequence is left out and the adjacent algorithms
          starting from the first in sequence are again compared. The first two algorithm $\mathbf{a}_2$ and
          $\mathbf{a}_1$ were already compared in Step 1. So now, the next comparison is $\mathbf{a}_1$ vs.~$\mathbf{a}_4$.
          Algorithm $\mathbf{a}_4$ results to be better than $\mathbf{a}_1$, so the algorithms are swapped as usual. However, the
          algorithms currently share the same rank. Since $\mathbf{a}_4$ reached the top of its performance
          class---having defeated all the other algorithms with the same rank---it should now get a higher rank than the
          other algorithms in its class. Therefore, the rank of $\mathbf{a}_1$ and $\mathbf{a}_3$ is incremented by 1
          (line \ref{lst:h1}-\ref{lst:h2} in Procedure \ref{alg:sort}): $\mathbf{a}_4$ stays at rank 2, and $\mathbf{a}_1$ and $\mathbf{a}_3$ receive rank 3. This completes the second pass of bubble sort and the two slowest algorithms have been pushed to the end of sequence.
	
	\item
          In the third and final pass,
          we again start from the first element in sequence and continue the pair-wise comparisons until the third last
          element. This leaves only one comparison to be done, $\mathbf{a}_4$ vs.~$\mathbf{a}_2$.
          Algorithm $\mathbf{a}_4$ is evaluated to be as good as $\mathbf{a}_2$, so both are given the same rank and the positions are not swapped. The ranks of algorithms occurring later than $\mathbf{a}_4$ in the sequence are decremented by 1 (this is the same rule applied in step 2). Thus,
          the final sequence is $\langle \mathbf{a}_2, \mathbf{a}_4, \mathbf{a}_1, \mathbf{a}_3 \rangle$. Algorithms $\mathbf{a}_2$ and $\mathbf{a}_4$ obtain rank 1, and $\mathbf{a}_1$ and $\mathbf{a}_3$ obtain rank 2.
\end{enumerate}

\begin{figure}
	\includegraphics[width=0.5\textwidth]{fig/ranking}
	\caption{Bubble Sort with Compare function}
	\label{fig:sort}
\end{figure}

All the algorithms with rank 1 are assigned to the set of fastest algorithms $\mathcal{F}$. From the above illustration,
$\mathcal{F} \leftarrow \{\mathbf{a_2}, \mathbf{a_4}\}$. Recall that the results of the Compare Function (Procedure \ref{alg:compare}) are not entirely deterministic due to non-transitivity of comparisons. As a consequence, the final ranks obtained by Procedure \ref{alg:sort} are also not entirely deterministic.
For instance, in step 5, algorithm $\mathbf{a_4}$ could be at the threshold  of being better than $\mathbf{a_2}$;
that is, if $\mathbf{a_4}$ were estimated to be better than $\mathbf{a_2}$ once in every two runs of Procedure \ref{alg:sort}, then fifty percent of the times
$\mathbf{a_2}$ would be pushed to rank 2 and would not be assigned to $\mathcal{F}$. To address this, the Sort function
is repeated
$T$ times and in each iteration all the algorithms that received rank 1 are accumulated in the list $L$. Then, we follow the steps from line 5-9 in Procedure \ref{alg:fa} to compute the relative score. If an
algorithm $\mathbf{a_j}$ was assigned rank 1 in $c$ out of $T$ iterations, then $\mathbf{a_j}$ would appear $c$ times in
the list $L$. Then, the relative confidence (or relative score) for $\mathbf{a_j}$ is $c/T$. 
%Notice that no bootstrapping is done in each of the $T$ iterations (as it was done in Procedure \ref{alg:fa}). The Sort function is repeated only to address the randomness in the final rankings and not to extract more information from the distribution (which is the objective of Bootstrapping that was addressed in the Compare function). 
The modified version of Procedure \ref{alg:fa} is shown in Procedure \ref{alg:f}, which also returns the set of fastest algorithms $\mathcal{F}$ (all the unique occurrences in $L$)
and their corresponding relative scores. Since we are interested only in the fastest algorithms, all the other
candidates that were not assigned rank 1 even once are given a relative score of 0. For the illustration in Figure 1, if $\mathbf{a_4} < \mathbf{a_2}$ in approximately one out of two runs of Sort function, $\mathbf{a_2}$ would get a relative score of 0.5 and $\mathbf{a_4}$ would get 1.0.  $\mathbf{a_1}$ and $\mathbf{a_3}$ would be given relative scores 0.


\begin{algorithm}
	\caption{ Get$\mathcal{F}$$(\mathcal{A})$ }
	\label{alg:f}
	\hspace*{\algorithmicindent} \textbf{Input: } $ \mathbf{a_1},\mathbf{a_2} ,\dots, \mathbf{a_p}\in \mathcal{A}$ \\
	\hspace*{\algorithmicindent} \textbf{Output: } $ (f_1,c_1), (f_2, c_2), \dots, (f_q,c_q) \in \mathcal{F} \times C  $
	\begin{algorithmic}[1] 
		\State $L, C \leftarrow [ \quad ]$ \Comment{Initialize empty lists}
		\For{i = 1, $\dots$, $T$}
		\State $[\langle (\mathbf{a}_j, r_j) | \forall j \rangle ] \leftarrow Sort(\mathcal{A})$ \Comment{$\forall j \in \{1,\dots,p\}$}
                \p{still struggling with this Sort. It would really be better if the name was immediately identified as
                  the sort from Procedure 3 -- I think we should have a slightly different name. SortBLAH. Then, we
                  would not struggle to denote the output, which right now is superconvoluted. Now that I get it, I feel
                  we need to replace $[\langle (\mathbf{a}_j, r_j) | \forall j \rangle ]$ with something easy to
                  digest. Maybe we don't need output at all, just:\\
                  $Sort(\mathcal{A})$\\
                  Select algs with rank 1\\
                 }
% \State $\tilde{\mathcal{F}} \leftarrow (\mathbf{s}_j | r_j = 1 )$ \Comment{select all algorithms with rank 1}
		\State select $\mathbf{a}_j$ such that $r_j = 1$
%                \p{should we use an action word such as "select( ... ) such that .. " ?}
%                \p{what are $r_j$?}
		\State append($\tilde{\mathcal{F}}, L $)
		\EndFor
		\State $\mathcal{F} \leftarrow $ unique elements in $L$ \Comment{$ q = |\mathcal{F}|$}
		\For{i = 1, $\dots$, q }
		\State $c \leftarrow$ number of occurrences of $f_i$ in $L$ $\quad f_i \in \mathcal{F}$
		\State append($c/T$, $C$)
		\EndFor
		\State return $(\mathcal{F} \times C)$
              \end{algorithmic}
              % \p{same comments apply. Do not use L.append, loops with "for i", syntax, ...}
\end{algorithm}

\section{Experiments}
\label{sec:exp}

For our experiments, we consider the solution algorithms generated by the Linnea
framework~\cite{barthels2019linnea}. For a given linear algebra expression with fixed matrix sizes, Linnea generates a
family of algorithms, coded in the Julia language~\cite{julia}.  Fig.~\ref{fig:d} presents the distributions of 50 time measurements\footnote{We
  use the measurement strategy described in Sec. \ref{sec:torel}. The measurements were taken on a dual  socket Intel
  Xeon E5-2680 v3 with 12 cores each and clock speed of 2.2 GHz running CentOS 7.7. Julia version 1.3 was linked against
  Intel MKL implementation of BLAS and LAPACK (MKL 2019 initial release).} of four equivalent algorithms $\mathbf{a}_0,
\mathbf{a}_1, \mathbf{a}_2, \mathbf{a}_3 \in \mathbb{R}^{50}$ for the ordinary least square problem  $(X^TX)^{-1}X^{T}y$
(pseudocode in Appendix A). The measurements were taken under two different settings. In the first setting, the number
of threads was fixed to 24; in the second setting, the number of threads for each repetition of an algorithm was
randomly chosen between 20 and 24. \p{should we give an intuition of why we did this? it is not apparent to me} In
Fig.~\ref{fig:d}, the distribution of $\mathbf{a}_0$ is compared against the distributions of the other algorithms.  The
distributions of $\mathbf{a}_0, \mathbf{a}_1, \mathbf{a}_2$ are largely overlapping; they all perform the same number of
floating point operations but differ in the order in which they execute matrix operations. Despite similar FLOP counts,
the order of computations can still affect the execution time due to cache influence between sequence of calls within
the algorithm\cite{peise2014cache}. In order to validate our methodology, we chose an example where similar FLOP counts
result in nearly identical distributions.
Algorithm $\mathbf{a}_3$ does twice the number of FLOPs than rest of the algorithms and hence noticeable difference in the distribution is observed.
\begin{figure}
	\centering
	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=1\linewidth]{fig/f_noise}
		\caption{Setting 1 : Number of threads is set to 24}
		\label{fig:Ng1}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=1\linewidth]{fig/f}
		\caption{Setting 2 : Threads are randomly chosen between 20 - 24}
		\label{fig:Ng2} 
	\end{subfigure}
	\caption{Fifty measurements of algorithms 1,2,3 compared against algorithm 0.  Algorithm 0: Blue;  Algorithm 1: Orange; Algorithm 2: Yellow; Algorithm 3: Red.}
	\label{fig:d}
\end{figure}

Recall that the relative scores \p{``produced/returned by Procedure 4''?} indicate the chance of an algorithm to be
assigned to the set of fast algorithms $\mathcal{F}$. In this example, one would naturally expect high scores for
$\mathbf{a}_0, \mathbf{a}_1$ and $\mathbf{a}_2$. The assignments made to $\mathcal{F}$ are said to be robust or
consistent or reproducible when they not largely influenced by operation setting \p{what do you mean with ``operation
  setting''?} or when the number of measurements is reduced (from 50).

\subsection{Robustness to Operation settings}

In the introduction, we stated that rankings 
that are based exclusively on a single statistic (such as minimum or mean
execution time) to quantify performance
lead to inconsistency when the measurements are reproduced.
\p{``reproduced'' is really a bad word, as it has a different meaning associated to it.
  Can we say instead elaborate along these lines?\\
  we stated that if algorithms are ranked according to a single performance number (``statistic''?), such as minimum or mean
  execution time, then the resulting ranking is highly sensitive to small changes to the input data. In other words, if
  new data (``measurements'', or ``timings'') were acquired, it would result in a different ranking with a high likelihood.}
Table \ref{tab:2} shows the distribution statistics of the algorithms $\mathbf{a}_0, \mathbf{a}_1, \mathbf{a}_2,
\mathbf{a}_3$ for the two different execution settings. For setting 1, when considering the minimum execution time,
$\mathbf{a}_1$ emerges as the best algorithm. However, if mean execution time is considered, then $\mathbf{a}_1$ is
distinctly behind $\mathbf{a}_0$ and $\mathbf{a}_2$. Furthermore, when the measurements are reproduced \p{not reproduced!!
  --  ``taken'', ``produced'', ``generated''}  under setting 2, both the minimum and the mean of $\mathbf{a}_1$ are somewhat
worse than those of $\mathbf{a}_0$ and $\mathbf{a}_2$.
Hence, it is not clear whether or not $\mathbf{a}_1$ should be assigned to $\mathcal{F}$.
\begin{table}[h!]
	\begin{center}
		\renewcommand{\arraystretch}{1.2}
		\begin{tabular}{@{}r rrr c rrr@{}}
			\toprule
			& \multicolumn{3}{c}{Setting 1} & & \multicolumn{3}{c}{Setting 2} \\
			\cmidrule{2-4} \cmidrule{6-8}
			& min & mean & std && min & mean & std \\
			\midrule
			{$\mathbf{a}_0$ \hfill }& 1.46  & 1.76  & 0.24  && 1.47  & 1.83  & 0.24  \\
			{$\mathbf{a}_1$ } & 1.44  & 1.82   & 0.25  && 1.51  & 1.91   & 0.44  \\
			{$\mathbf{a}_2$ } & 1.46  & 1.75  & 0.28  && 1.45  & 1.85  & 0.24  \\
			{$\mathbf{a}_3$ } & 1.74  & 2.61  & 0.73  && 2.05  & 2.6  & 0.36  \\
			\bottomrule
		\end{tabular}
		\caption{Time statistics for the four algorithms (in ms).}
		\label{tab:2}
	\end{center}
\end{table}

\p{I feel that this point is very important, and should appear already in the intro, and possibly in the
  conclusion. In a nutshell, you are saying that work is an essential step to train performance models, as it generates
  consistent/reliable ground truth.}
One could argue that, despite the inconsistency, the best algorithm that is identified with a single statistic is indeed
always one of the fastest algorithms ($\mathbf{a}_0$ or $\mathbf{a}_1$ or $\mathbf{a}_2$). However, often in practice the
best algorithm has  to be identified without executing them; for this, realistic performance models are needed. Such
performance models are usually trained and evaluated against certain ground truth; in our example, the distributions of
$\mathbf{a}_0$, $\mathbf{a}_1$, $\mathbf{a}_2$ are indeed nearly identical and it is essential to know the true chances
that every algorithm have in order to be assigned to $\mathcal{F}$. To this end, the consistency of performance estimate
is important. Furthermore,
in Sec.~\ref{sec:con} we describe
practical use-cases for which it is important to identify not one, but all the fast algorithms.
 
In Sec.~\ref{sec:torel}, we then introduced the simple bootstrapping method in Procedure \ref{alg:fa} and argued that it does not take into account the uncertainty in measurement data and therefore the test for significant difference is necessary, in which one algorithm is evaluated to be faster than the other only when there is enough evidence. We then modified Procedure \ref{alg:fa} by incorporating the test for significant difference using the three-way Compare function (Procedure \ref{alg:compare}), which is used to sort and rank algorithms into equivalence classes in Procedure \ref{alg:sort}. 
 
In Procedure \ref{alg:compare}, when $M = 1$, the three-way Compare function falls back to a regular comparison with
two outcomes (``better'' vs.~''worse''); as a consequence, the Get$\mathcal{F}$ function (Procedure \ref{alg:f}) reduces to the basic bootstrapping method
described in Sec.~\ref{sec:torel} (Procedure \ref{alg:fa}).
Table \ref{tab:1} shows the relative scores obtained by Procedure \ref{alg:fa} ($M=1$  and $threshold$ not applicable) and Procedure
\ref{alg:f} (for different values of $threshold$ with $M=30$).
% \p{Maybe it would be a good idea to repeat the meaning of
%  these scores}
%The experiments were also repeated by randomly switching between 20 to 24 threads to simulate system noise.
% The distributions of the algorithms in comparison with Algorithm 0  are shown in Figure \ref{fig:d}. 
 %Significant overlap can be observed between algorithms 0,1 and 2. 

 		% \begin{tabular}{c|ccc|ccc|}
 		% 	\cline{2-7}
 		% 	& \multicolumn{3}{|c|}{Without Noise} &  \multicolumn{3}{|c|}{With Noise} \\
 		% 	\cline{2-7}
 		% 	& Alg 0 & Alg 1 & Alg 2 & Alg 0 & Alg 1 & Alg 2 \\
 		% 	\hline
 		% 	\multicolumn{1}{ |c| }{M=1, thresh=N/A}  & 0.45 &0.11 &0.44 & 0.53 & 0.07 & 0.40 \\
 		% 	\hline
 		% 	\multicolumn{1}{ |c| }{M=30, thresh=0.50} & 0.53 &0.0 &0.47 & 0.76 & 0.0 & 0.24 \\
 		% 	\hline
 		% 	\multicolumn{1}{ |c| }{M=30, thresh=0.80} & 0.96 &0.73 &0.90 & 0.97 & 0.12 & 0.95\\
 		% 	\hline
 		% 	\multicolumn{1}{ |c| }{M=30, thresh=0.85}& 0.97 &0.89 &0.94 & 0.95 & 0.22 & 0.93 \\
 		% 	\hline
 		% 	\multicolumn{1}{ |c| }{M=30, thresh=0.90} & 0.99 &0.97 &0.99 & 0.95 & 0.66 & 0.90\\
 		% 	\hline
 		% 	\multicolumn{1}{ |c| }{M=30, thresh=0.95}& 1.0 &0.99 &0.99 & 0.97 & 0.88 & 0.97 \\
 		% 	\hline
 		% \end{tabular}
%\p{for all tables, please use tabbing and this next one as a template}
\begin{table}[h!]
  \begin{center}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{@{}r rrr c rrr@{}}
      \toprule
      & \multicolumn{3}{c}{Setting 1} & & \multicolumn{3}{c}{Setting 2} \\
      \cmidrule{2-4} \cmidrule{6-8}
                       & $\mathbf{a}_0$ & $\mathbf{a}_1$  & $\mathbf{a}_2$  && $\mathbf{a}_0$  & $\mathbf{a}_1$  & $\mathbf{a}_2$  \\
      \midrule
      {M=1, \hfill thr=N/A}
                       & 0.45  & 0.11  & 0.44  && 0.53  & 0.07  & 0.40  \\
      {M=30, thr=0.50} & 0.53  & 0.0   & 0.47  && 0.76  & 0.0   & 0.24  \\
      {M=30, thr=0.80} & 0.96  & 0.73  & 0.90  && 0.97  & 0.12  & 0.95  \\
      {M=30, thr=0.85} & 0.97  & 0.89  & 0.94  && 0.95  & 0.22  & 0.93  \\
      {M=30, thr=0.90} & 0.99  & 0.97  & 0.99  && 0.95  & 0.66  & 0.90  \\
      {M=30, thr=0.95} & 1.0   & 0.99  & 0.99  && 0.97  & 0.88  & 0.97  \\
      \bottomrule
    \end{tabular}
    \caption{$T$ = 500, $K$ = 10}
    \label{tab:1}
  \end{center}
\end{table}

For $M=1$, $\mathbf{a}_0$, $\mathbf{a}_1$ and $\mathbf{a}_2$ were assigned to  $\mathcal{F}$ and $\mathbf{a}_1$ received a low relative score in both the settings. Therefore, based on the analysis from $N=50$ time measurements, $\mathbf{a}_1$ lagged slightly behind $\mathbf{a}_0$ and $\mathbf{a}_2$. However, 50 measurements are still only a snapshot of the true distribution, which is not available. Therefore, the test for significant
difference is performed and the result of two comparisons is considered equivalent ($\sim$) if there is not enough evidence
that one algorithm dominates the other. The tolerance level up to which two algorithms should be considered equivalent
is controlled by adjusting the $threshold$ parameter in the Compare function. When $threshold=0.5$, the tolerance level
is 0 and the outcome $\sim$ in the Compare function is still impossible. When the relative scores are computed with the
setting  $M=30$ and $threshold=0.5$, $\mathbf{a}_0$ obtained the score 0 and was strictly not assigned to $\mathcal{F}$.
When $threshold$ is increased, the conditions for one algorithm to be ranked better than the other in Procedure
\ref{alg:sort} become stricter and as a result, it becomes difficult for $\mathbf{a}_0$ and $\mathbf{a}_2$ to be ranked
strictly better than $\mathbf{a}_1$. Therefore, as $threshold$ increased, the relative score of $\mathbf{a}_1$ also
increases (see Table \ref{tab:1}), which means that it was assigned with rank 1 more frequently across different
repetitions of the Sort function in Procedure \ref{alg:f}. $\mathbf{a}_3$ still obtains a relative score of 0; this is
because its difference from the other distributions is noticeable.


The ``tolerance" level is also impacted by the parameter $K$ in Procedure \ref{alg:compare}, which is the number of
measurements sampled in each of the $M$ bootstrap iteration. In every iteration of comparing \p{``every iteration of
  comparing''? I don't get it}  two algorithms $(\mathbf{a_i}, \mathbf{a_j})$, the minimum execution time $(e_i,e_j)$ of
the samples ($\mathbf{\tilde{a}_i} \subset \mathbf{a_i}$, $\mathbf{\tilde{a}_j} \subset \mathbf{a_j}$) is computed. As
$K \to N (=50)$, the minimum of the samples $(\mathbf{\tilde{a}_i},\mathbf{\tilde{a}_j)}$ would approximate \p{I don't
  like ``approximate''; ``tend to''? ``approach''?} the minimum of the distributions $(\mathbf{a_i}, \mathbf{a_j})$. As a consequence, when $K=50$, the evaluation of $e_i \le e_j$ in the Compare function becomes deterministic and $\sim$ would become an impossible outcome. For Setting 1 in Table \ref{tab:2}, $\mathbf{a}_1$ recorded the lowest execution time; therefore, as $K \to 50$, the relative score of $\mathbf{a}_1$ approaches 1.0, while the scores of $\mathbf{a}_0$ and $\mathbf{a}_2$ approach 0 (see Figure \ref{fig:k}). 
\begin{figure}[h!]
	\includegraphics[width=0.5\textwidth]{fig/k}
	\caption{Relative Score vs $K$ \\ $T$=500, $M$=30, $threshold$=0.9}
	\label{fig:k}     
\end{figure}
Thus, higher values of $K$ invalidate the advantages of bootstrapping as the rankings start depending more and more exclusively on a single statistic. It is ideal to have $K$ randomly chosen from a set of values (say $K \in [5,10]$).

\subsection{Robustness to number of measurements}

We evaluate the consistency or robustness of assignments made to $\mathcal{F}$ when the number of measurements $N$ of
each algorithm decreases. Let $\mathcal{F}_{N}$ be the set of fastest algorithms identified with $N$ measurements of
each algorithm. In Table~\ref{tab:3}, we report the average precision and recall (explained shortly) of
$\mathcal{F}_{N<50} $ with respect to $\mathcal{F}_{50}$ for linear algebra expressions from 25 application examples
(from \cite{barthels2019linnea}).
\p{This sentence is repeated later, I don't see the need here}

%We consider the solutions to be less robust when the precision is low. 
Consider an instance \p{instance of what? are we talking about one of those 25 examples?}
obtained with $M=1$ (Procedure \ref{alg:fa})
where
$\mathcal{F}_{50}:\{\mathbf{a}_0,\mathbf{a}_2\}$ and
$\mathcal{F}_{20}:\{\mathbf{a}_0,\mathbf{a}_1,\mathbf{a}_2,\mathbf{a}_3,\mathbf{a}_4\}$.
\p{is this hypothetical, or it comes from a specific example?} 
When $N$ is decreased to 20, one finds less and less evidence \p{what are we looking at? is there data for this?} of one
algorithm dominating the others; as a consequence, more algorithms manage to obtain rank 1 in at least one of the $T$
iterations of Procedure \ref{alg:fa} and are assigned to $\mathcal{F}_{20}$.  We assume that $\mathcal{F}_{50}$ is the
closer to the true solution than $\mathcal{F}_{N<50}$. When $\mathcal{F}_{20}$ is compared against $\mathcal{F}_{50}$,
the set $\mathcal{F}_{20}$ has  $\{\mathbf{a}_0,\mathbf{a}_2\}$ as true positives ($TP$),
$\{\mathbf{a}_1,\mathbf{a}_3,\mathbf{a}_4\}$  as false positives ($FP$) and there are no false negatives
($FN$). Therefore, the precision and recall for this example are 0.4 and 1.0, respectively.\footnote{Precision =
  $TP/(TP+FP)$. Recall = $TP/(TP+FN)$. \p{Maybe point out that $0\le prec \le 1$? same for recall}}

We argue that for robustness, precision is a more important evaluation metric than recall; when the precision is low but
recall is high, although all the fastest algorithms are identified at a lower $N$ (resulting in a high recall score),
$\mathcal{F}_{20}$ is cluttered with false positives. \p{Here we need something like ``Vice versa, blah blah'' or ``On
  the contrary,'' ...}  But for an instance $\mathcal{F}_{20} : \{\mathbf{a}_2\}$ obtained with $M=30$ and
$threshold=0.5$, there is one false negative $\{\mathbf{a}_0\}$ and no false positives; thus, the precision is 1.0 and
the recall is 0.5. Although not all the fastest algorithms are identified (resulting in a lower recall score), there are
no false positives and hence the solutions are precise. A low recall is still not ideal when consistent solutions are expected, but precision is a more important evaluation metric. 
\begin{table}[h!]
	\begin{center}
		\renewcommand{\arraystretch}{1.2}
		\begin{tabular}{@{}r rr c rr c rr c rr@{}}
			\toprule
			& \multicolumn{8}{c}{$M=30$} & & \multicolumn{2}{c}{$M=1$} \\
			\cmidrule{2-9} \cmidrule{11-12}
			& \multicolumn{2}{c}{thr=0.9} & & \multicolumn{2}{c}{thr=0.8} & & \multicolumn{2}{c}{thr=0.5} & & \multicolumn{2}{c}{thr=NA} \\
			\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12}
			{$N$} & \textbf{prc} & \textbf{rec} && \textbf{prc} & \textbf{rec} && \textbf{prc} & \textbf{rec} && \textbf{prc} & \textbf{rec} \\
			\midrule
			{40} & 0.97  & 0.94  && 0.86  & 0.90  && 0.71  & 0.67 && 0.32 & 0.99 \\
			{35} & 0.95  & 0.94  && 0.91  & 0.87  && 0.78  & 0.65 && 0.31 & 0.99 \\
			{30} & 0.93  & 0.86  && 0.88  & 0.87  && 0.68  & 0.58 && 0.34 & 0.99 \\
			{25} & 0.95  & 0.86  && 0.90  & 0.78  && 0.58  & 0.58 && 0.34 & 0.98 \\
			{20} & 0.97  & 0.80  && 0.93  & 0.75  && 0.50  & 0.38 && 0.36 & 0.95 \\
			{15} & 0.98  & 0.59  && 0.96  & 0.61  && 0.69  & 0.29 && 0.44 & 0.85 \\
			\bottomrule
		\end{tabular}
		\caption{Average Precision and Recall of $\mathcal{F}$  \\ $T$=50, $K$=10}
		\label{tab:3}
	\end{center}
\end{table}
Table \ref{tab:3} shows the precision and recall values averaged for 25 different linear algebra expressions (taken from
\cite{barthels2019linnea}) arising in practical applications. \p{repetition from the first paragraph -- kill here or there} It can be seen that the precision improves considerably when $M=30$ even with zero tolerance (i.e., $threshold = 0.5$). Furthermore, both precision and recall improve (even for low $N$) as $threshold$ increases.
 
Recall however that high values of $threshold$ imply stricter conditions for one algorithm to be ranked better than the
other; this means thst the outcome $\sim$ in the Compare function becomes more and more likely and as a result more
algorithms get clustered with the same rank even when there is a marginal overlap of distributions. For example, at
$M=30$ and $threshold=0.5$,  $\mathcal{F}_{50}$: $\{(\mathbf{a}_0,1.0),(\mathbf{a}_2,1.0)\}$ (the second element in the
tuple is relative score); when $threshold$ is increased to 0.9, $\mathcal{F}_{50}$:
$\{(\mathbf{a}_0,1.0),(\mathbf{a}_1,0.8), (\mathbf{a}_2,1.0), (\mathbf{a}_3,0.9)\}$, i.e., two more  algorithms are assigned to $\mathcal{F}_{50}$, with fairly high relative scores. When $threshold = 0.8$, $\mathcal{F}_{50}$: $\{(\mathbf{a}_0,1.0),(\mathbf{a}_1,0.3), (\mathbf{a}_2,1.0), (\mathbf{a}_3,0.5)\}$, $\mathbf{a}_1$ and $\mathbf{a}_3$ are still assigned but their relative scores decrease. Therefore, as $threshold$ becomes too high, discriminating algorithms based on relative scores (chance of being the fastest algorithm) becomes more difficult.
 
% \ar{Aravind got here}
% 
% \section{Why is Relative Performance important?}
% \label{sec:app}
% In the past decade, we have witnessed an increasing number of latency sensitive applications involving intelligent vehicles\cite{connectedvehicles}, real-time video analytics\cite{videoanalytics}, augmented reality\cite{arvr} etc.,  that require costly scientific computations on resource-constrained hardware. Offloading all the computations to a cloud is not a viable solution for such applications because of unacceptable communication latency between the cloud and end devices\cite{surveyMCC} \cite{towardsEdgeComputing}. On the other hand, increasing capabilities of mobile devices enable them to be more suitable for scientific computing\cite{smartPhonesForScientificComputing} \cite{raspberryEdgeComputing}. Consequently, there is a trend of computation offloading\cite{surveyOfComputationalOffloading2013} toward edge computing \cite{edgeComputing2016} \cite{towardsEdgeComputing} \cite{edgeComputing2015}, where significant gains are realized when computations are shifted towards the edge of the network (including the local device)\cite{EdgeComputingQuantifying}. For instance, a proof-of-concept platform that runs face recognition application in \cite{facerecog} shows that the response time is reduced from 900 to 169 ms by moving computation from cloud to the edge. 
%% Depending on the distribution of workload among devices, one could come up with myriads of implementation for the same computational problem, each having a significant impact on both latency and energy consumption. Clone cloud in \cite{clonecloud} does on-demand partitioning of workload between mobile and the cloud, and their prototype could reduce 20x running time and energy for the tested application.
% 
% Increasing number of such applications in recent times can be attributed to containerization tools like Docker\cite{docker} that ease portability of code without compromising performance\cite{dockerForEdgeComputing} \cite{dockerhpc} and enable the use of same Linux environment across heterogeneous devices. Further more, recent high level programming languages like Julia\cite{julia}, Tensorflow\cite{tensorflow}, PyTorch\cite{pytorch} etc that abstract the calls to linear algebra libraries optimized for different hardware (such as raspberry Pi, GPU), allow developers to write code without worrying about the underlying architecture and yet achieve close to machine performance. While benchmarking suites like \cite{cavbench}\cite{edgeaibench} are used to evaluate performance over the network, it is also important to find out if the single node devices also achieve the required performance. Linear algebra computations, which appear at the heart of most scientific computations can have hundreds of equivalent implementations, each  with different performance footprints\cite{barthels2019linnea} and therefore selection of the right implementation is equally significant.
 
\section{Conclusion and Future Outlook}
\label{sec:con}
We presented a methodology to cluster and rank a set of equivalent algorithms into performance classes using a
measurement-based approach that assigns a relative score to the algorithms in comparison to one another. We showed that
the rankings are robust to operation settings \p{same comemnt as before: I don't understand what operation settings
  are. ``environment''? ``computing environment'' ``computing conditions''?}
and can be consistent in identifying the fast algorithms even when the
number of measurements are decreased.\footnote{Code: https://github.com/as641651/Relative-Performance}


Linear algebra operations occur at the heart of many mathematical computations and are one of the major computational
bottlenecks.  In the past decade, we have witnessed an increasing number of latency-sensitive applications involving
intelligent vehicles\cite{connectedvehicles}, real-time video analytics\cite{videoanalytics}, augmented
reality\cite{arvr} etc., that require costly scientific computations on resource-constrained hardware. for these
applications, the common approach
of offloading all the computations to a cloud is no longer a fully viable solution because of unacceptable communication
latency between the cloud and end devices~\cite{surveyMCC,towardsEdgeComputing}. Consequently, there is a trend toward
``edge computing"~\cite{edgeComputing2016,towardsEdgeComputing,edgeComputing2015}, where computations are shifted more
and more towards the end devices, which include smartphones, GPUs, Raspberry Pi, etc. For instance, a proof-of-concept
platform that runs a face recognition application shows that the response time is reduced from 900 to
169 ms by moving computation from cloud to the edge~\cite{facerecog}, or \p{``or'' is this still from the same paper? otherwise
  the sentence does not make sense}  the resilience of a flying drone to atmospheric conditions can be improved if the on-board processor can compute a few extra gradient per second. Hence efficient computation of linear algebra operations, especially on devices that are usually subject to noisy operational setting and limited compute resources, are becoming more and more relevant.  

%significant gains are realized when computations are shifted towards the edge of the network (including the local device)\cite{EdgeComputingQuantifying}.
The typical development of linear algebra code involves a lot of trial and error in choosing the best implementation out
of several possible alternatives. In order to automate code development, languages such as Julia\cite{julia}, Matlab\cite{MatlabOTB}, Tensorflow\cite{tensorflow} etc., were developed to automatically decompose a mathematical expression into sequences of library calls. Compilers like Linnea~\cite{barthels2019linnea} were developed on top of such high level languages to identify several alternative algorithms for the same mathematical expression. The selection of best algorithm can be guided by models that predict relative performance. Therefore, a natural extension to this work would be "Relative Performance modelling", where we aim to automatically predict (or model) the relative scores without having to execute all the algorithms.

Our methodology can be applied to applications beyond the scope of linear algebra; as we do not make any assumption about the distributions of execution times.  For instance, based on the split of computational workload among several resources, one could come up with myriads of implementation for the same computational problem, each having a significant impact on several factors like latency, energy consumption etc,\cite{clonecloud}. Moreover, when code optimization is done with respect to additional factors like energy, identifying more than one fast algorithms becomes helpful. 

\bibliographystyle{IEEEtran}
\bibliography{references}

%\clearpage
\section*{Appendix}
\subsection{Equivalents Algorithms for Ordinary Least Square Problem}
\label{sec:appA}
\begin{algorithm}
	\renewcommand{\thealgorithm}{}
	\floatname{algorithm}{Algorithm 0}
	\caption{ Blue }
	\label{alg:a0}
	\textbf{Expression: } $(X^TX)^{-1}X^{T}y \qquad X \in \mathbb{R}^{1000 \times 500} \quad y \in \mathbb{R}^{500}$ 
	\begin{algorithmic}[1] 
		\State $T_1 \leftarrow syrk(X^{T}X)$ \Comment{$T_1^{-1}X^{T}y$}
		\State $LL^{T} \leftarrow $ Cholesky($T_1$) \Comment{$L^{-T}L^{-1}X^{T}y$}
		\State $t_2 = X^{T}y$ \Comment{$L^{-T}L^{-1}t_2$}
		\State $t_2 = L^{-1}t_2$ \Comment{$L^{-T}t_2$}
		\State $z = L^{-T}t_2$
	\end{algorithmic}
\end{algorithm}

In \textbf{Algorithm 1} and \textbf{Algorithm 2}, the order in which computations are evaluated is changed, but they have the same FLOP count  as \textbf{Algorithm 0}. Still, the order of computations can affect the execution time due of cache effects between sequence of calls\cite{peise2014cache}.
\begin{algorithm}
	\renewcommand{\thealgorithm}{}
	\floatname{algorithm}{Algorithm 1}
	\caption{ Orange }
	\label{alg:a0}
	\textbf{Expression: } $(X^TX)^{-1}X^{T}y \qquad X \in \mathbb{R}^{1000 \times 500} \quad y \in \mathbb{R}^{500}$ 
	\begin{algorithmic}[1] 
		\State $t_1 = X^{T}y$ \Comment{$(X^{T}X)^{-1}t_1$}
		\State $T_2 \leftarrow syrk(X^{T}X)$ \Comment{$T_2^{-1}t_1$}
		\State $LL^{T} \leftarrow $ Cholesky($T_2$) \Comment{$L^{-T}L^{-1}t_1$}
		\State $t_1 = L^{-1}t_1$ \Comment{$L^{-T}t_1$}
		\State $z = L^{-T}t_1$
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\renewcommand{\thealgorithm}{}
	\floatname{algorithm}{Algorithm 2}
	\caption{ Yellow }
	\label{alg:a0}
	\textbf{Expression: } $(X^TX)^{-1}X^{T}y \qquad X \in \mathbb{R}^{1000 \times 500} \quad y \in \mathbb{R}^{500}$ 
	\begin{algorithmic}[1] 
		\State $T_1 \leftarrow syrk(X^{T}X)$ \Comment{$T_1^{-1}X^{T}y$}
		\State $t_2 = X^{T}y$ \Comment{$T_1^{-1}t_2$}
		\State $LL^{T} \leftarrow $ Cholesky($T_1$) \Comment{$L^{-T}L^{-1}t_2$}
		\State $t_2 = L^{-1}t_2$ \Comment{$L^{-T}t_2$}
		\State $z = L^{-T}t_2$
	\end{algorithmic}
\end{algorithm}

The FLOP count for \textbf{Algorithm 3} is 2x more than  \textbf{Algorithm 0, 1, 2}.
\begin{algorithm}[H]
	\renewcommand{\thealgorithm}{}
	\floatname{algorithm}{Algorithm 3}
	\caption{ Red }
	\label{alg:a0}
	\textbf{Expression: } $(X^TX)^{-1}X^{T}y \qquad X \in \mathbb{R}^{1000 \times 500} \quad y \in \mathbb{R}^{500}$ 
	\begin{algorithmic}[1] 
		\State $T_1 \leftarrow gemm(X^{T}X)$ \Comment{$T_1^{-1}X^{T}y$}
		\State $LL^{T} \leftarrow $ Cholesky($T_1$) \Comment{$L^{-T}L^{-1}X^{T}y$}
		\State $t_2 = X^{T}y$ \Comment{$L^{-T}L^{-1}t_2$}
		\State $t_2 = L^{-1}t_2$ \Comment{$L^{-T}t_2$}
		\State $z = L^{-T}t_2$
	\end{algorithmic}

\end{algorithm}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
